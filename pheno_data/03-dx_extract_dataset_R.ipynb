{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: dx extract_dataset for R\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# “dx extract_dataset” in R\n",
        "<hr/>\n",
        "***As-Is Software Disclaimer***\n",
        "\n",
        "This content in this repository is delivered “As-Is”. Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "This notebook demonstrates usage of the dx command `extract_dataset` for:\n",
        "* Retrieval of Apollo-stored data, as referenced within entities and fields of a Dataset or Cohort object on the platform\n",
        "* Retrieval of the underlying data dictionary files used to generate a Dataset object on the platform\n",
        "\n",
        "<a href=\"https://github.com/dnanexus/OpenBio/blob/master/LICENSE.md\">MIT License</a> applies to this notebook.\n",
        "\n",
        "## Preparing your environment\n",
        "### Launch spec:\n",
        "\n",
        "* App name: JupyterLab with Python, R, Stata, ML ()\n",
        "* Kernel: R\n",
        "* Instance type: Spark Cluster - mem1_ssd1_v2_x2, 3 nodes \n",
        "* Snapshot: `/.Notebook_snapshots/jupyter_snapshot.gz`\n",
        "* Cost: < $0.2\n",
        "* Runtime: =~ 10 min\n",
        "* Data description: Input for this notebook is a v3.0 Dataset or Cohort object ID\n",
        "\n",
        "### Install dxpy\n",
        "extract_dataset requires dxpy version >= 0.329.0. If running the command from your local environment (i.e. off of the DNAnexus platform), it may be required to also install pandas. For example, pip3 install -U dxpy[pandas]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "system(\"pip3 show dxpy\", intern = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install tidyverse for data processing\n",
        "\n",
        "Quick note - you will need to read the licenses for the tidyverse in order to make sure whether you and your group are comfortable with the licensing terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "#| eval: false\n",
        "install.packages(c(\"readr\", \"stringr\", \"dplyr\", \"glue\", \"reactable\", \"janitor\", \"remotes\"))\n",
        "remotes::install_github(\"laderast/xvhelper\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "library(dplyr)\n",
        "library(readr)\n",
        "library(stringr)\n",
        "library(glue)\n",
        "library(reactable)\n",
        "library(xvhelper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Assign environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# The referenced Dataset is private and provided only to demonstrate an example input. The user will need to supply a permissible and valid record-id\n",
        "\n",
        "# Assign project-id of dataset\n",
        "# In general, you should use the project id of your UKB project\n",
        "\n",
        "# Assign dataset record-id\n",
        "# record id can be either from a dataset or from a cohort\n",
        "projectid <- \"project-XXXXXX\"\n",
        "rid <- \"record-G406j8j0x8kzxv3G08k64gVV\"\n",
        "# Assign joint dataset project-id:record-id\n",
        "dataset <- glue::glue(\"{projectid}:{rid}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Call “dx extract_dataset” using a supplied dataset\n",
        "\n",
        "We'll use the `{glue}` package to put our bash commands together for `dx extract_dataset`, and use `system()` to execute our bash code.\n",
        "\n",
        "`glue::glue()` has the advantage of not needing to `paste()` together strings. The string substitution is cleaner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "cmd <- glue::glue(\"dx extract_dataset {dataset} -ddd\")\n",
        "\n",
        "cmd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's execute our command using `system()` and then we will list the files that result using `list.files()`. We generate three files in the directory in JupyterLab storage:\n",
        "\n",
        "- *dataset_name*`.codings.csv`\n",
        "- *dataset_name*`.data_dictionary.csv`\n",
        "- *dataset_name*`.entity_dictionary.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "system(cmd)\n",
        "list.files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preview data in the three dictionary (*.csv) files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "#codings_file <- system(\"ls *.codings.csv\", intern = TRUE)\n",
        "codings_file <- list.files(pattern=\"*.codings.csv\")\n",
        "codings_df <- read_csv(codings_file, show_col_types = FALSE)\n",
        "head(codings_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "entity_dict_file <- system(\"ls *.entity_dictionary.csv\", intern=TRUE)\n",
        "entity_dict_df <- read_csv(entity_dict_file, show_col_types = FALSE)\n",
        "head(entity_dict_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Data Dictionary File\n",
        "\n",
        "The data dictionary is the glue for the entire dataset. It maps:\n",
        "\n",
        "- Entity to Fields\n",
        "- Fields to Codings\n",
        "- Entity to Entity\n",
        "\n",
        "We'll use the data dictionary to understand how to building our list of fields, and later, we'll join it to the codings file to build a list of fields and their coded values.\n",
        "\n",
        "There are more columns to the data dictionary, but let's first see the `entity`, `name`, `title`, and `type` columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "#data_dict_file <- system(\"ls *.data_dictionary.csv\", intern=TRUE)\n",
        "data_dict_file <- list.files(pattern=\"*.data_dictionary.csv\")\n",
        "data_dict_df <- read_csv(data_dict_file, show_col_types = FALSE)\n",
        "data_dict_df <- data_dict_df \n",
        "\n",
        "data_dict_df %>%\n",
        "        select(entity, name, title, type) %>%\n",
        "        head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Parse returned metadata and extract entity/field names\n",
        "\n",
        "Let's search for some fields. We want the following fields:\n",
        "\n",
        "- `Coffee intake | instance 0`\n",
        "- `Sex` (Gender)\n",
        "- `Smoked cigarette or pipe within last hour | Instance 0`\n",
        "\n",
        "We can use the `{reactable}` package to make a searchable table of the data dictionary. This will help in finding fields.\n",
        "\n",
        "Note the search box in the top right of the table - when we have many fields, we can use the search box to find fields of interest. Try searching for `Coffee intake` and see what fields pop up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "data_dict_df <- data_dict_df %>%\n",
        "    relocate(name, title) %>%\n",
        "    mutate(ent_field = glue::glue(\"{entity}.{name}\"))\n",
        "\n",
        "basic_data_dict <- data_dict_df |>\n",
        "                    select(title, name, entity, ent_field, coding_name, is_multi_select, is_sparse_coding)\n",
        "\n",
        "reactable::reactable(basic_data_dict, searchable = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another strategy for searching fields: we can use `grepl` within `dplyr::filter()` to search for fields that match our criteria.\n",
        "\n",
        "Note we're chaining the `grepl` statements with an OR `|`.\n",
        "\n",
        "We're also concatenating `entity` and `name` to a new variable, `ent_field`, which we'll use when we specify our list of fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "filtered_dict <- data_dict_df %>%\n",
        "    filter(grepl(\"Coffee type\", title) | \n",
        "           grepl(\"Sex\", title) | \n",
        "           grepl(\"Smoked\", title) | \n",
        "           grepl(\"Age at recruitment\", title) |\n",
        "           grepl(\"main ICD10\", title) |\n",
        "           grepl(\"Types of transport\", title)\n",
        "          ) %>%\n",
        "    arrange(title) \n",
        "\n",
        "filtered_dict %>%\n",
        "    select(name, title, ent_field)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use this subset of fields - we'll pull the `ent_field` column, and paste it together into a single comma delimited string using `paste`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "field_list <- filtered_dict %>%\n",
        "    pull(ent_field)\n",
        "\n",
        "#field_list <- field_list[200:210]\n",
        "field_list <- paste(field_list, collapse = \",\")\n",
        "field_list <- paste0(\"participant.eid,\", field_list)\n",
        "field_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Use extracted entity and field names as input to the called function, “dx extract_dataset” and extract data\n",
        "\n",
        "Again, we'll use `glue()` here for cleaner string substitution.\n",
        "\n",
        "We'll extract the cohort information to a file called `cohort_data.csv` and work with this file for the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "cohort_template <- \"dx extract_dataset {dataset} --fields {field_list} -o cohort_data.csv\"\n",
        "cmd <- glue::glue(cohort_template)\n",
        "\n",
        "cmd\n",
        "\n",
        "system(cmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preview data in the retrieved data file\n",
        "\n",
        "We'll see that the retrieved data contains the integer and character codes. These must be decoded (see below):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "data_df <- read_csv(\"cohort_data.csv\", show_col_types = FALSE)\n",
        "head(data_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding columns with xvhelper\n",
        "\n",
        "xvhelper is a little R package that will return the actual values of the returned data.\n",
        "\n",
        "To use it, you build a `coded_col_df` using `merge_coding_data_dict()` and then translate the categorical columns to values using `decode_categories()`, and then change the column names to R friendly clean ones using `decode_column_names()`.\n",
        "\n",
        "Note that we need to run `decode_df()` before we run `decode_category()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "#| eval: false\n",
        "\n",
        "#install via remotes::install_github()\n",
        "#install.packages(\"remotes\")\n",
        "remotes::install_github(\"laderast/xvhelper\")\n",
        "\n",
        "library(xvhelper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "coded_col_df <- xvhelper::merge_coding_data_dict(coding_dict = codings_df, data_dict = data_dict_df)\n",
        "\n",
        "decoded <- data_df %>%\n",
        "    xvhelper::decode_single(coded_col_df) |>\n",
        "    xvhelper::decode_multi_purrr(coded_col_df) |>\n",
        "    xvhelper::decode_column_names(coded_col_df, r_clean_names = FALSE)\n",
        "    \n",
        "head(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(ggplot2)\n",
        "\n",
        "ggplot(decoded) + \n",
        "  aes(x=`Coffee type | Instance 0`) +\n",
        "  geom_bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "decoded |>\n",
        "  dplyr::select(`Participant ID`, `Diagnoses - main ICD10`) |>\n",
        "  tidyr::separate_rows(`Diagnoses - main ICD10`, sep=\"\\\\|\") |>\n",
        "  filter(grepl(\"Chapter\", `Diagnoses - main ICD10`)) |>\n",
        "  count(`Diagnoses - main ICD10`) |>\n",
        "  arrange(desc(n)) |>\n",
        "  gt::gt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "decoded |>\n",
        "  dplyr::select(`Participant ID`, `Diagnoses - main ICD10`) |>\n",
        "  tidyr::separate_rows(`Diagnoses - main ICD10`, sep=\"\\\\|\") |>\n",
        "  filter(grepl(\"Chapter\", `Diagnoses - main ICD10`)) |>\n",
        "  ggplot() + aes(x=`Diagnoses - main ICD10`) + geom_bar() + \n",
        "  theme(axis.text.x = element_text(angle = 90))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "#| eval: false\n",
        "write.csv(decoded, file=\"cohort_decoded.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Output to Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "#| eval: false\n",
        "system(\"dx upload *.csv --destination /users/tladeras/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "language": "R",
      "display_name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}